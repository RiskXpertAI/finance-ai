import hashlib
from http.client import HTTPException
from app.logger import *
from fastapi import HTTPException
from fastapi.responses import StreamingResponse
import torch
from openai import OpenAI
from starlette.staticfiles import StaticFiles

from app.indicator import fetch_and_store_financial_data
from app.models import TextRequest, ChatRequest

from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse


from fastapi import FastAPI, Form, Request

from app.server_predict import load_recent_data, predict_nth_future, TimeSeriesLSTMModel
from app.services import get_openai_response, save_generated_text, build_forecast_prompt
from app.redis_cache import cache_forecast, get_cached_forecast, call_prediction_api
from starlette.responses import JSONResponse
from app.routes import protected
from app.routes.auth import router as auth_router, KAKAO_CLIENT_ID, KAKAO_REDIRECT_URI  # ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸ ë¼ìš°í„°
from app.utils.slack_alert import send_slack_alert

app = FastAPI()
client=OpenAI()

# ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸
app.include_router(auth_router)

# âœ… ë³´í˜¸ëœ ë¼ìš°í„° ë“±ë¡
app.include_router(protected.router)

templates = Jinja2Templates(directory="templates")

from app.health import api_router as health_router

app.include_router(health_router)

@app.get("/status")
async def read_root():
    return {"message": "MongoDB ì—°ê²° ì„±ê³µ!"}


# Text ìƒì„± API
@app.post("/generate_text/")
async def generate_text_and_save(request: TextRequest):
    logging.info(f"[Generate Text] ìš”ì²­ ì‹œì‘ | Prompt: {request.prompt}")

    try:
        generated_text = await get_openai_response(request.prompt)
        logging.info(f"[Generate Text] ì‘ë‹µ ì™„ë£Œ | {generated_text}")

        await save_generated_text(request.prompt, generated_text)
        logging.info("[Generate Text] MongoDB ì €ì¥ ì™„ë£Œ")

        return {"generated_text": generated_text}

    except Exception as e:
        logging.error(f"[Generate Text] ì²˜ë¦¬ ì‹¤íŒ¨ | Error: {e}")
        raise HTTPException(status_code=500, detail="í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨")

# âœ… ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ API
@app.post("/fetch-financial-data/")
async def fetch_financial_data():
    """ í•œêµ­ì€í–‰ APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ MongoDBì— ì €ì¥í•˜ëŠ” API """
    response = await fetch_and_store_financial_data()
    return response

""" --------------------------------front_end--------------------------------------"""



# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory="static"), name="static")

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# mainpage.html ë Œë”ë§ ("/main" ë¼ìš°íŠ¸ ëŒ€ì‘)
@app.get("/main", response_class=HTMLResponse)
async def render_mainpage(request: Request):
    return templates.TemplateResponse("mainpage.html", {"request": request})
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("index.html", {
        "request": request,
        "client_id": KAKAO_CLIENT_ID,
        "redirect_uri": KAKAO_REDIRECT_URI
    })



# FastAPI predict ì—”ë“œí¬ì¸íŠ¸
import traceback

@app.post("/predict")
async def predict(months: int = Form(...), window_size: int = Form(12)):
    try:
        feature_cols = [
            "GDP", "í™˜ìœ¨", "ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜", "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜", "ê¸ˆë¦¬",
            "GDP_MA3", "GDP_DIFF", "GDP_PCT",
            "í™˜ìœ¨_MA3", "í™˜ìœ¨_DIFF", "í™˜ìœ¨_PCT",
            "ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜_MA3", "ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜_DIFF", "ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜_PCT",
            "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜_MA3", "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜_DIFF", "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜_PCT",
            "ê¸ˆë¦¬_MA3", "ê¸ˆë¦¬_DIFF", "ê¸ˆë¦¬_PCT"
        ]
        target_cols = ["GDP", "í™˜ìœ¨", "ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜", "ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜", "ê¸ˆë¦¬"]

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = TimeSeriesLSTMModel(
            input_size=len(feature_cols),
            output_size=len(target_cols),
            hidden_size=256,
            num_layers=2,
            dropout=0.2
        )
        model.load_state_dict(torch.load("lstm_model.pt", map_location=device))

        recent_input, target_scaler, last_time = load_recent_data(feature_cols, target_cols, window_size)
        pred_scaled = predict_nth_future(model, recent_input, device=device)
        pred_scaled_2d = pred_scaled.reshape(1, -1)
        pred_real = target_scaler.inverse_transform(pred_scaled_2d).flatten().astype(float)


        # ë¯¸ë˜ TIME ê³„ì‚°
        base_year = int(str(last_time)[:4])
        base_mon = int(str(last_time)[4:])
        target_mon = base_mon + months
        target_year = base_year
        while target_mon > 12:
            target_mon -= 12
            target_year += 1
        future_time = f"{target_year}{target_mon:02d}"

        result = {**dict(zip(target_cols, pred_real)), "TIME": future_time}

        return JSONResponse(content=result)

    except Exception as e:
        logging.error(f"âŒ ì˜ˆì¸¡ ì—ëŸ¬: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.post("/chat/stream/")
async def stream_chat(request: ChatRequest):
    user_input = request.prompt
    months = request.months
    print("months: ",months)

    logging.info(f"[Chat Stream] ìš”ì²­ ì‹œì‘ | Prompt: {user_input} | Months: {months}")

    question_hash = hashlib.md5(user_input.encode()).hexdigest()
    cache_key = f"forecast_{months}_months_{question_hash}"

    cached_response = get_cached_forecast(cache_key)
    if cached_response:
        logging.info(f"[Chat Stream] ìºì‹œ ë°ì´í„° í™œìš© | CacheKey: {cache_key}")

        async def stream_cached():
            for c in cached_response:
                yield c
        return StreamingResponse(stream_cached(), media_type="text/plain")

    try:
        forecast = await call_prediction_api(months, 12)
        logging.info(f"[Chat Stream] ì˜ˆì¸¡ ë°ì´í„° ìƒì„± ì„±ê³µ")

        prompt = build_forecast_prompt(user_input, forecast,months)
        gpt_response = await get_openai_response(prompt)
        logging.info(f"[Chat Stream] GPT ìš”ì•½ ì‘ë‹µ ì™„ë£Œ")

        async def generate_final_response():
            full_response = ""
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": gpt_response}],
                stream=True
            )
            for chunk in response:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    yield content

            cache_forecast(cache_key, full_response)
            await save_generated_text(user_input, full_response)
            logging.info(f"[Chat Stream] ìµœì¢… ì‘ë‹µ ìºì‹œ ë° ì €ì¥ ì™„ë£Œ")

        return StreamingResponse(generate_final_response(), media_type="text/plain")

    except Exception as e:
        logging.error(f"[Chat Stream] ì²˜ë¦¬ ì‹¤íŒ¨ | Error: {e}")
        raise HTTPException(status_code=500, detail="ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨")

# test_slack_alert
@app.get("/test/slack")
async def test_slack_alert():
    send_slack_alert("ğŸ”¥ ìŠ¬ë™ ì•Œë¦¼ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤", level="ERROR")
    return {"message": "ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡ ì‹œë„ ì™„ë£Œ"}


import hashlib
import logging

import httpx
from pydantic import ValidationError
from starlette.staticfiles import StaticFiles

from app.indicator import fetch_and_store_financial_data
from app.models import TextRequest
from app.database import database

from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse


from fastapi import FastAPI, HTTPException, Form, Request
from app.services import get_openai_response, save_generated_text, build_forecast_prompt, get_scenario_based_answer
from app.redis_cache import cache_forecast, get_cached_forecast, call_prediction_api
from app.transformer import run_forecasting, ChatRequest
from starlette.responses import JSONResponse
from app.routes import protected
from app.routes.auth import router as auth_router, KAKAO_CLIENT_ID, KAKAO_REDIRECT_URI  # ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸ ë¼ìš°í„°

logging.basicConfig(level=logging.DEBUG)

app = FastAPI()


# ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸
app.include_router(auth_router)

# âœ… ë³´í˜¸ëœ ë¼ìš°í„° ë“±ë¡
app.include_router(protected.router)

templates = Jinja2Templates(directory="templates")


@app.get("/status")
async def read_root():
    return {"message": "MongoDB ì—°ê²° ì„±ê³µ!"}


@app.post("/generate_text/")
async def generate_text_and_save(request: TextRequest):
    collection = database.get_collection("generated_texts")  # collection ê°€ì ¸ì˜¤ê¸°

    print(f"ğŸ”µ OpenAI API ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤: {request.prompt}")  # ìš”ì²­ ë¡œê·¸ í™•ì¸ìš© print ì¶”ê°€
    generated_text = await get_openai_response(request.prompt)  # OpenAI API í˜¸ì¶œ
    print(f"ğŸŸ¢ OpenAI API ì‘ë‹µ ë°›ìŒ: {generated_text}")  # ì‘ë‹µ í™•ì¸

    await save_generated_text(request.prompt, generated_text)  # MongoDB ì €ì¥
    return {"generated_text": generated_text}

# âœ… ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ API
@app.post("/fetch-financial-data/")
async def fetch_financial_data():
    """ í•œêµ­ì€í–‰ APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ MongoDBì— ì €ì¥í•˜ëŠ” API """
    response = await fetch_and_store_financial_data()
    return response

""" --------------------------------front_end--------------------------------------"""

templates = Jinja2Templates(directory="templates")  # HTML íŒŒì¼ì´ ë“¤ì–´ê°ˆ í´ë” ì§€ì •
app.mount("/static", StaticFiles(directory="static"), name="static")


# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory="static"), name="static")

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# mainpage.html ë Œë”ë§ ("/main" ë¼ìš°íŠ¸ ëŒ€ì‘)
@app.get("/main", response_class=HTMLResponse)
async def render_mainpage(request: Request):
    return templates.TemplateResponse("mainpage.html", {"request": request})
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("index.html", {
        "request": request,
        "client_id": KAKAO_CLIENT_ID,
        "redirect_uri": KAKAO_REDIRECT_URI
    })


@app.post("/predict")
async def predict(months: int = Form(...), window_size: int = Form(12)):
    try:
        result = run_forecasting(window_size=window_size, forecast_horizon=months)
        return JSONResponse(content=result)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})



# @app.post("/chat/")
# async def chat(request: ChatRequest):
#     user_input = request.prompt  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
#     months = request.months  # ì„ íƒí•œ ê°œì›” ìˆ˜
#
#     # 1. transformer ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸° (httpx ì‚¬ìš©)
#     async with httpx.AsyncClient() as client:  # httpx.AsyncClient ì‚¬ìš©
#         predict_response = await client.post(
#             # "http://localhost:8000/predict",  # ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ë¡œ ìˆ˜ì • í•„ìš”
#             "http://host.docker.internal:8000/predict",  # ë„ì»¤ ë‚´ì—ì„œ ì™¸ë¶€ API í˜¸ì¶œ ì‹œ ìˆ˜ì • í•„ìš”
#
#             data={"months": months, "window_size": 12}
#         )
#
#     if predict_response.status_code != 200:
#         raise HTTPException(status_code=500, detail="ì˜ˆì¸¡ ì˜¤ë¥˜")
#
#     forecast = predict_response.json()  # ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
#     forecast_text = "\n".join([f"{k}: {v:.2f}" for k, v in forecast.items() if k != "TIME"])
#
#     # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = build_forecast_prompt(user_input, forecast)
#
#     # 3. GPTì— ìš”ì²­
#     response_text = await get_openai_response(prompt)
#     await save_generated_text(user_input, response_text)
#
#     return {"response": response_text}  # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ


# Redis ìºì‹œì—ì„œ ì˜ˆì¸¡ê°’ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
@app.post("/chat/")
async def chat(request: ChatRequest):
    try:
        user_input = request.prompt  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
        months = request.months  # ì„ íƒí•œ ê°œì›” ìˆ˜
        logging.debug(f'user_input: {user_input}, months: {months}')
    except ValidationError as e:
        logging.debug(f"Validation error: {e.errors()}")  # ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ ì¶œë ¥
        raise HTTPException(status_code=422, detail=f"Validation error: {e.errors()}")

    # ì„œë²„ì—ì„œ ë°ì´í„° í™•ì¸ì„ ìœ„í•œ ì¶”ê°€ ë¡œê·¸
    logging.debug(f"User input: {user_input}, months: {months}")

    # ì§ˆë¬¸ì„ í•´ì‹œí™”í•˜ì—¬ ìºì‹œ í‚¤ë¥¼ ë‹¤ë¥´ê²Œ ìƒì„±
    question_hash = hashlib.md5(user_input.encode()).hexdigest()  # ì§ˆë¬¸ì„ í•´ì‹œí™”
    cache_key = f"forecast_{months}_months_{question_hash}"  # ìºì‹œ í‚¤ì— ì§ˆë¬¸ í•´ì‹œ ì¶”ê°€
    logging.debug(f"ğŸ” Redis ìºì‹œì—ì„œ ì˜ˆì¸¡ê°’ì„ í™•ì¸ ì¤‘... ìºì‹œ í‚¤: {cache_key}")

    forecast = get_cached_forecast(cache_key)
    if forecast:
        # Redisì— ì˜ˆì¸¡ê°’ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        logging.debug(f"ğŸ”µ Redisì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ê°’ ë¡œë“œ ì„±ê³µ: {forecast}")
        return forecast  # ì˜ˆì¸¡ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜ (GPT ë¡œì§ì€ ëŒë¦¬ì§€ ì•ŠìŒ)

    else:
        # Redisì— ì˜ˆì¸¡ê°’ì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ APIë¥¼ í˜¸ì¶œí•˜ê³ , GPT ë¡œì§ì„ ëŒë¦°ë‹¤.
        logging.debug(f"ğŸ”´ Redisì— ìºì‹œëœ ì˜ˆì¸¡ê°’ì´ ì—†ì–´ì„œ ìƒˆë¡œìš´ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        forecast = await call_prediction_api(months, 12)  # ì˜ˆì¸¡ API í˜¸ì¶œ
        logging.debug(f"ğŸŸ¢ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤: {forecast}")

        # ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•˜ì—¬ GPT ì‘ë‹µ ìƒì„±
        prompt_for_gpt = build_forecast_prompt(user_input, forecast)
        logging.debug(f"ğŸ”µ ìƒì„±ëœ GPT í”„ë¡¬í”„íŠ¸: {prompt_for_gpt}")

        # GPT-1 ì‘ë‹µ ìƒì„±
        gpt_response = await get_openai_response(prompt_for_gpt)
        logging.debug(f"ğŸŸ¢ GPT-1 ì‘ë‹µ: {gpt_response}")

        # GPT-2ë¡œ ì¶”ê°€ ë‹µë³€ ìƒì„±
        final_response = await get_scenario_based_answer(gpt_response)
        logging.debug(f"ğŸŸ¢ GPT-2 ì‘ë‹µ: {final_response}")

        # ìµœì¢… GPT ì‘ë‹µì„ Redisì— ì €ì¥
        cache_forecast(cache_key, final_response)  # ìµœì¢… GPT ì‘ë‹µì„ Redisì— ìºì‹œ
        await save_generated_text(user_input, final_response)  # MongoDBì— ì €ì¥

        return final_response
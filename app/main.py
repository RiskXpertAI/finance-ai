import asyncio
import hashlib
import logging
from fastapi.responses import StreamingResponse

from openai import OpenAI
from pydantic import ValidationError
from starlette.staticfiles import StaticFiles

from app.indicator import fetch_and_store_financial_data
from app.models import TextRequest
from app.database import database

from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse


from fastapi import FastAPI, HTTPException, Form, Request
from app.services import get_openai_response, save_generated_text, build_forecast_prompt, get_scenario_based_answer
from app.redis_cache import cache_forecast, get_cached_forecast, call_prediction_api, get_redis_client
from app.transformer import run_forecasting, ChatRequest
from starlette.responses import JSONResponse
from app.routes import protected
from app.routes.auth import router as auth_router, KAKAO_CLIENT_ID, KAKAO_REDIRECT_URI  # ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸ ë¼ìš°í„°

logging.basicConfig(level=logging.DEBUG)

app = FastAPI()
client=OpenAI()

# ì¹´ì¹´ì˜¤ ë¡œê·¸ì¸
app.include_router(auth_router)

# âœ… ë³´í˜¸ëœ ë¼ìš°í„° ë“±ë¡
app.include_router(protected.router)

templates = Jinja2Templates(directory="templates")


@app.get("/health/mongo")
async def mongo_health_check():
    try:
        await database.command("ping")
        return {"status": "ok", "msg": "MongoDB connected"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health/redis")
async def redis_health_check():
    try:
        r = await get_redis_client()
        await r.ping()
        return {"status": "ok", "msg": "Redis connected"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def read_root():
    return {"message": "MongoDB ì—°ê²° ì„±ê³µ!"}


@app.post("/generate_text/")
async def generate_text_and_save(request: TextRequest):
    collection = database.get_collection("generated_texts")  # collection ê°€ì ¸ì˜¤ê¸°

    print(f"ğŸ”µ OpenAI API ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤: {request.prompt}")  # ìš”ì²­ ë¡œê·¸ í™•ì¸ìš© print ì¶”ê°€
    generated_text = await get_openai_response(request.prompt)  # OpenAI API í˜¸ì¶œ
    print(f"ğŸŸ¢ OpenAI API ì‘ë‹µ ë°›ìŒ: {generated_text}")  # ì‘ë‹µ í™•ì¸

    await save_generated_text(request.prompt, generated_text)  # MongoDB ì €ì¥
    return {"generated_text": generated_text}

# âœ… ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ API
@app.post("/fetch-financial-data/")
async def fetch_financial_data():
    """ í•œêµ­ì€í–‰ APIì—ì„œ ê¸ˆìœµ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ MongoDBì— ì €ì¥í•˜ëŠ” API """
    response = await fetch_and_store_financial_data()
    return response

""" --------------------------------front_end--------------------------------------"""

templates = Jinja2Templates(directory="templates")  # HTML íŒŒì¼ì´ ë“¤ì–´ê°ˆ í´ë” ì§€ì •
app.mount("/static", StaticFiles(directory="static"), name="static")


# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory="static"), name="static")

# í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# mainpage.html ë Œë”ë§ ("/main" ë¼ìš°íŠ¸ ëŒ€ì‘)
@app.get("/main", response_class=HTMLResponse)
async def render_mainpage(request: Request):
    return templates.TemplateResponse("mainpage.html", {"request": request})
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("index.html", {
        "request": request,
        "client_id": KAKAO_CLIENT_ID,
        "redirect_uri": KAKAO_REDIRECT_URI
    })


@app.post("/predict")
async def predict(months: int = Form(...), window_size: int = Form(12)):
    try:
        result = run_forecasting(window_size=window_size, forecast_horizon=months)
        return JSONResponse(content=result)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/chat/stream/")
async def stream_chat(request: ChatRequest):
    user_input = request.prompt
    months = request.months

    question_hash = hashlib.md5(user_input.encode()).hexdigest()
    cache_key = f"forecast_{months}_months_{question_hash}"
    cached_response = get_cached_forecast(cache_key)

    if cached_response:
        async def stream_cached():
            for c in cached_response:
                yield c
        return StreamingResponse(stream_cached(), media_type="text/plain")

    # 1. ì˜ˆì¸¡ê°’ ìƒì„±
    forecast = await call_prediction_api(months, 12)
    prompt = build_forecast_prompt(user_input, forecast)

    # 2. GPT-1 í˜¸ì¶œ (ìš”ì•½ í”„ë¡¬í”„íŠ¸)
    gpt_response = await get_openai_response(prompt)

    # 3. GPT-2 í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
    async def generate_final_response():
        full_response = ""
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": gpt_response}],
            stream=True
        )

        for chunk in response:
            content = chunk.choices[0].delta.content
            if content:
                full_response += content
                yield content

        # âœ… ìºì‹œì™€ Mongo ì €ì¥ (await ê°€ëŠ¥!)
        cache_forecast(cache_key, full_response)
        await save_generated_text(user_input, full_response)

    return StreamingResponse(generate_final_response(), media_type="text/plain")